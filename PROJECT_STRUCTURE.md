# Project Structure

This document outlines the organization of the LLM Evaluation Framework project.

## ğŸ“ Directory Structure

```
llm_evaluator/
â”œâ”€â”€ README.md                   # Main project documentation
â”œâ”€â”€ LICENSE                     # MIT license
â”œâ”€â”€ CHANGELOG.md               # Version history and changes
â”œâ”€â”€ CONTRIBUTING.md            # Contribution guidelines
â”œâ”€â”€ pyproject.toml             # Modern Python packaging configuration
â”œâ”€â”€ setup.py                   # Legacy Python packaging (for compatibility)
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ Makefile                   # Development automation scripts
â”œâ”€â”€ .gitignore                 # Git ignore rules
â”œâ”€â”€ .env.example              # Environment variables template
â”œâ”€â”€ config.yaml               # Main configuration file
â”œâ”€â”€ sample_test_suite.json    # Example test cases
â”œâ”€â”€ main.py                   # Legacy CLI entry point
â”œâ”€â”€ cli.py                    # Modern CLI interface
â”‚
â”œâ”€â”€ src/                      # Main source code package
â”‚   â”œâ”€â”€ __init__.py          # Package initialization
â”‚   â”œâ”€â”€ models.py            # Data models (TestCase, ModelResponse, etc.)
â”‚   â”œâ”€â”€ llm_models.py        # LLM provider implementations
â”‚   â”œâ”€â”€ config.py            # Configuration management
â”‚   â”œâ”€â”€ metrics.py           # Evaluation metrics
â”‚   â”œâ”€â”€ cache.py             # Response caching system
â”‚   â”œâ”€â”€ utils.py             # Utilities and logging
â”‚   â””â”€â”€ exceptions.py        # Custom exception classes
â”‚
â”œâ”€â”€ tests/                   # Test suite
â”‚   â””â”€â”€ test_framework.py   # Unit tests
â”‚
â”œâ”€â”€ docs/                    # Documentation (empty, for future use)
â”œâ”€â”€ results/                 # Evaluation results (gitignored)
â””â”€â”€ wandb/                   # Weights & Biases logs (gitignored)
```

## ğŸ§© Module Organization

### Core Modules

- **`src/models.py`**: Data classes for the framework
  - `ModelResponse`: API response wrapper
  - `TestCase`: Individual test case
  - `EvaluationResult`: Complete evaluation result

- **`src/llm_models.py`**: LLM provider implementations
  - `BaseModel`: Abstract base class
  - `OpenAIModel`: OpenAI API integration
  - `AnthropicModel`: Anthropic Claude integration
  - `GeminiModel`: Google Gemini integration
  - `OllamaModel`: Local Ollama integration

- **`src/metrics.py`**: Evaluation metrics
  - `BLEUCalculator`: BLEU score computation
  - `ROUGECalculator`: ROUGE score computation
  - `BERTScoreCalculator`: BERTScore computation
  - `SemanticSimilarityCalculator`: Sentence transformer similarity
  - `CategorySpecificMetrics`: Task-specific evaluation

### Support Modules

- **`src/config.py`**: Configuration management
  - `ConfigManager`: Load and validate configurations
  - `ConfigValidator`: Validate configuration structure

- **`src/cache.py`**: Caching system
  - `ResultsCache`: Cache API responses and metrics

- **`src/utils.py`**: Utilities
  - `PerformanceMonitor`: Track performance metrics
  - `RateLimiter`: API rate limiting
  - Logging setup and formatting

- **`src/exceptions.py`**: Custom exceptions
  - Framework-specific error types

## ğŸ”§ Configuration Files

### Main Configuration (`config.yaml`)
- Model provider settings
- API key references
- Pricing information
- Evaluation parameters

### Development Configuration (`pyproject.toml`)
- Package metadata
- Dependencies
- Development tools configuration
- Build system settings

### Environment Variables (`.env`)
- API keys and secrets
- Workspace-specific settings

## ğŸ“Š Data Flow

1. **Configuration Loading**: `ConfigManager` loads and validates `config.yaml`
2. **Model Creation**: `ModelFactory` creates provider instances
3. **Test Loading**: `TestSuite` loads test cases from JSON/CSV
4. **Evaluation**: `ModelEvaluator` runs async evaluation
5. **Metrics**: Individual calculators compute scores
6. **Results**: Output to CSV files and W&B dashboard

## ğŸ¯ Entry Points

### CLI Interface (`cli.py`)
Modern Click-based interface with commands:
- `init-config`: Create default configuration
- `init-tests`: Create sample test suite
- `run`: Execute evaluation
- `validate`: Validate configuration
- `results`: Display results summary

### Legacy Interface (`main.py`)
Original argparse-based interface for backward compatibility

## ğŸ§ª Testing Strategy

### Unit Tests (`tests/test_framework.py`)
- Configuration validation
- Model initialization
- Metric calculations
- Error handling

### Integration Tests (Future)
- End-to-end evaluation workflows
- API provider integration
- Performance benchmarks

## ğŸ“ˆ Monitoring & Logging

### Performance Monitoring
- API response times
- Token usage tracking
- Cost calculations
- Error rates

### Logging Levels
- `DEBUG`: Detailed execution flow
- `INFO`: Key operations and results
- `WARNING`: Non-fatal issues
- `ERROR`: Failures and exceptions

## ğŸ”„ Development Workflow

### Code Style
- **Black**: Code formatting
- **isort**: Import sorting
- **flake8**: Linting
- **mypy**: Type checking

### Automation (`Makefile`)
- `make format`: Format code
- `make lint`: Run linting
- `make test`: Execute tests
- `make install-dev`: Development setup

## ğŸ“¦ Packaging

### Distribution
- **PyPI**: Standard Python package
- **GitHub**: Source code repository
- **Docker**: Containerized deployment (future)

### Dependencies
- **Core**: Runtime dependencies
- **Dev**: Development tools
- **Optional**: Provider-specific packages

## ğŸ”’ Security

### API Keys
- Environment variable storage
- `.env` file template
- Gitignored sensitive files

### Data Privacy
- No API responses stored in git
- Configurable cache directory
- Optional result encryption (future)

## ğŸš€ Deployment

### Local Development
1. Clone repository
2. Install dependencies
3. Configure environment
4. Run evaluations

### Production
- Environment-specific configurations
- Secrets management
- Monitoring integration
- Result persistence

---

*This structure supports scalability, maintainability, and ease of contribution while following Python best practices.*
